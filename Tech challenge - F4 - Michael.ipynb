{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1GYa4YKF3Nt_DbJzZ0nytnPt-S_h0jt3I","authorship_tag":"ABX9TyPjmgRi7unO+Fl82RUbF4sz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"FQoalqigUZ7J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739639288431,"user_tz":180,"elapsed":17715,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}},"outputId":"37fecbc3-6461-4af3-facb-2e07f0e51848"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-contrib-python) (1.26.4)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Collecting deepface\n","  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.32.3)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.26.4)\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.2)\n","Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n","Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.67.1)\n","Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.1.0)\n","Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.11.0.86)\n","Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n","Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\n","Collecting flask-cors>=4.0.1 (from deepface)\n","  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n","Collecting mtcnn>=0.1.0 (from deepface)\n","  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n","Collecting retina-face>=0.0.1 (from deepface)\n","  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n","Collecting fire>=0.4.0 (from deepface)\n","  Downloading fire-0.7.0.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gunicorn>=20.1.0 (from deepface)\n","  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.5)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (3.17.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.12.1)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.14.0)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n","Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n","Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n","  Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2025.1.31)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.25.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.70.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n","Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n","Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n","Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=75885f8f70fb1255ea873a72f5a633ed7d38e3881754a57bae81a99518ada6cd\n","  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n","Successfully built fire\n","Installing collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\n","Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.0 gunicorn-23.0.0 lz4-4.4.3 mtcnn-1.0.0 retina-face-0.0.17\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n","Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n","Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n","Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n","Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n","Installing collected packages: sounddevice, mediapipe\n","Successfully installed mediapipe-0.10.21 sounddevice-0.5.1\n"]}],"source":["!pip install opencv-python\n","!pip install opencv-contrib-python\n","!pip install tensorflow\n","!pip install deepface\n","!pip install mediapipe"]},{"cell_type":"code","source":["# Configuração inicial para o Colab\n","from google.colab import drive\n","from google.colab.patches import cv2_imshow\n","\n","# Montar o Google Drive para salvar o resultado (opcional)\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozXRDbCDlPHN","executionInfo":{"status":"ok","timestamp":1739639378672,"user_tz":180,"elapsed":13155,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}},"outputId":"4cc76794-fdfb-47a6-d32e-21c7d027cb47"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import cv2\n","import matplotlib.pyplot as plt\n","from deepface import DeepFace\n","import time\n","from tqdm import tqdm\n","import numpy as np\n","\n","# Verificação da versão do OpenCV\n","print(f\"Versão do OpenCV: {cv2.__version__}\")\n","\n","# Definição das emoções reconhecidas\n","emotions = {'angry': 0, 'disgust': 0, 'fear': 0, 'happy': 0, 'sad': 0, 'surprise': 0, 'neutral': 0}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSCxgqClZkWy","executionInfo":{"status":"ok","timestamp":1739641061873,"user_tz":180,"elapsed":5638,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}},"outputId":"b516bd96-5625-4068-a3b0-46cfe693c92e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Versão do OpenCV: 4.11.0\n"]}]},{"cell_type":"code","source":["def detect_faces_with_rotation(frame, face_detector):\n","    \"\"\"\n","    Detecta rostos em um frame, girando a imagem em múltiplos ângulos para lidar com rostos virados.\n","\n","    Args:\n","        frame (numpy.ndarray): O frame de vídeo.\n","        face_detector: O objeto detector de rostos YuNet.\n","\n","    Returns:\n","        list: Uma lista de rostos detectados (se houver), ou None se nenhum rosto for detectado.\n","    \"\"\"\n","    detections = None\n","\n","    for angle in [0, 90, 180, 270]:\n","        if angle != 0:\n","            (h, w) = frame.shape[:2]\n","            center = (w // 2, h // 2)\n","            M = cv2.getRotationMatrix2D(center, angle, 1.0)\n","            rotated = cv2.warpAffine(frame, M, (w, h))\n","        else:\n","            rotated = frame.copy()  # Crie uma cópia para evitar modificar o original\n","\n","        face_detector.setInputSize((rotated.shape[1], rotated.shape[0]))\n","        _, faces = face_detector.detect(rotated)\n","\n","        if faces is not None:\n","            # Ajustar as coordenadas das caixas delimitadoras para a imagem original\n","            if angle != 0:\n","                for face in faces:\n","                    x1, y1, w, h, confidence = map(int, face[:5])\n","                    # Desfazer a rotação das coordenadas\n","                    pts = np.array([[[x1, y1]], [[x1 + w, y1]], [[x1 + w, y1 + h]], [[x1, y1 + h]]], dtype=np.float32)\n","                    rotated_pts = cv2.transform(pts, M[:2])\n","                    x1_rotated = int(min(rotated_pts[0][0][0], rotated_pts[1][0][0], rotated_pts[2][0][0], rotated_pts[3][0][0]))\n","                    y1_rotated = int(min(rotated_pts[0][0][1], rotated_pts[1][0][1], rotated_pts[2][0][1], rotated_pts[3][0][1]))\n","                    w_rotated = int(max(rotated_pts[0][0][0], rotated_pts[1][0][0], rotated_pts[2][0][0], rotated_pts[3][0][0])) - x1_rotated\n","                    h_rotated = int(max(rotated_pts[0][0][1], rotated_pts[1][0][1], rotated_pts[2][0][1], rotated_pts[3][0][1])) - y1_rotated\n","\n","                    face[:5] = [x1_rotated, y1_rotated, w_rotated, h_rotated, face[4]]  # Atualiza as coordenadas no array 'face'\n","\n","            detections = faces\n","            break  # Pare de girar se encontrar algum rosto\n","\n","    return detections\n","\n","def process_video(video_path, output_path=\"output.mp4\"):\n","    \"\"\"\n","    Detecta rostos e emoções em um vídeo, utilizando YuNet para detecção de rostos e DeepFace para análise de emoções.\n","\n","    Args:\n","        video_path (str): Caminho para o arquivo de vídeo de entrada.\n","        output_path (str, optional): Caminho para o arquivo de vídeo de saída com as detecções e emoções. Defaults to \"output.avi\".\n","    \"\"\"\n","    # 1. Inicialização do Modelo YuNet para Detecção de Rostos\n","    modelPath = \"/content/drive/MyDrive/Colab Notebooks/F4/face_detection_yunet_2023mar.onnx\"\n","    face_detector = cv2.FaceDetectorYN_create(\n","        modelPath,\n","        \"\",  # Modelo de configuração (vazio para padrão)\n","        (640, 640),    # input_size (ajuste conforme necessário) - Changed to a tuple\n","        0.75,  # scoreThreshold (ajuste conforme necessário)\n","        0.1,  # nmsThreshold (ajuste conforme necessário)\n","        0 # Device ID (0 para CPU)\n","    )\n","\n","    # 2. Configuração do Vídeo de Entrada e Saída\n","    video = cv2.VideoCapture(video_path)\n","    if not video.isOpened():\n","        print(f\"Erro ao abrir o vídeo: {video_path}\")\n","        return\n","\n","    frame_width = int(video.get(3))\n","    frame_height = int(video.get(4))\n","    fps = int(video.get(cv2.CAP_PROP_FPS))\n","    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # ou 'XVID' dependendo do codec disponível\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n","\n","    # 3. Loop Principal de Processamento do Vídeo\n","    frame_count = 0\n","    start_time = time.time()\n","\n","    for _ in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n","        ret, frame = video.read()\n","        if not ret:\n","            break\n","\n","        frame_count += 1\n","\n","        # Define a resolução do detector de rostos\n","        face_detector.setInputSize((frame.shape[1], frame.shape[0]))\n","\n","        # 4. Detecção de Rostos com YuNet\n","        faces = detect_faces_with_rotation(frame, face_detector)\n","\n","        if faces is not None:\n","            # 5. Análise de Emoções com DeepFace para cada rosto detectado\n","            for face in faces:\n","                # Extrai as coordenadas do rosto\n","                x1, y1, w, h, confidence = map(int, face[:5])\n","\n","                # Garante que as coordenadas estejam dentro dos limites da imagem\n","                x1 = max(0, x1)\n","                y1 = max(0, y1)\n","                w = min(w, frame.shape[1] - x1)\n","                h = min(h, frame.shape[0] - y1)\n","\n","                # Extrai a região do rosto para análise de emoção\n","                face_img = frame[y1:y1+h, x1:x1+w]\n","\n","                try:\n","                    # Analisa a emoção usando DeepFace\n","                    analysis = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False, detector_backend='skip')\n","                    # print(f\"Resultado da análise: {analysis}\")  # Imprime o resultado completo\n","                    dominant_emotion = analysis[0]['dominant_emotion']\n","\n","                    # Desenha a caixa delimitadora e a emoção no frame\n","                    cv2.rectangle(frame, (x1, y1), (x1 + w, y1 + h), (0, 255, 0), 2)\n","                    cv2.putText(frame, dominant_emotion, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n","                    emotions[dominant_emotion] += 1\n","\n","                except Exception as e:\n","                    # print(f\"Erro ao analisar emoção: {e}\")\n","\n","                    print(f\"Erro ao analisar emoção: {type(e).__name__}: {e}\")  # Imprime o tipo e a mensagem de erro\n","\n","                    cv2.rectangle(frame, (x1, y1), (x1 + w, y1 + h), (0, 0, 255), 2)\n","                    cv2.putText(frame, \"Erro\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n","\n","\n","        # 6. Salva o frame processado no vídeo de saída\n","        out.write(frame)\n","\n","    # 7. Liberação dos recursos\n","    video.release()\n","    out.release()\n","    cv2.destroyAllWindows()\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(f\"Vídeo processado: {output_path}\")\n","    print(f\"Tempo total de processamento: {elapsed_time:.2f} segundos\")\n","    print(f\"Número total de frames: {frame_count}\")\n","    print(f\"FPS médio: {frame_count / elapsed_time:.2f}\")\n","    print(f\"Emoções detectadas: {emotions}\")\n","    for emotion, count in emotions.items():\n","        percentage = (count / frame_count) * 100\n","        print(f\"{emotion}: {percentage:.2f}%\")"],"metadata":{"id":"E3OA64NlVPgD","executionInfo":{"status":"ok","timestamp":1739639405450,"user_tz":180,"elapsed":1016,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Caminhos para vídeo de entrada/saída\n","input_video_path = \"/content/drive/MyDrive/Colab Notebooks/F4/input_shorter.mp4\"  # Substituir pelo caminho do vídeo carregado\n","output_video_path = \"/content/drive/MyDrive/Colab Notebooks/F4/output_shorter.mp4\"\n","\n","\n","# Execute a função para analisar o vídeo\n","process_video(input_video_path, output_video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmEIn8Ds-Jms","executionInfo":{"status":"ok","timestamp":1739628727649,"user_tz":180,"elapsed":37331,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}},"outputId":"487c9aa8-0b38-4bd4-e0ce-34a5e6988bbb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Processando vídeo: 100%|██████████| 291/291 [00:37<00:00,  7.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Vídeo processado: /content/drive/MyDrive/Colab Notebooks/F4/output_shorter.mp4\n","Tempo total de processamento: 37.21 segundos\n","Número total de frames: 291\n","FPS médio: 7.82\n","Emoções detectadas: {'angry': 186, 'disgust': 0, 'fear': 471, 'happy': 570, 'sad': 660, 'surprise': 9, 'neutral': 480}\n","Porcentagem de cada emoção: {'angry': 186, 'disgust': 0, 'fear': 471, 'happy': 570, 'sad': 660, 'surprise': 9, 'neutral': 480}\n","angry: 63.92%\n","disgust: 0.00%\n","fear: 161.86%\n","happy: 195.88%\n","sad: 226.80%\n","surprise: 3.09%\n","neutral: 164.95%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import mediapipe as mp\n","import torch\n","from mediapipe.python.solutions import pose as mp_pose\n","from tqdm import tqdm\n","\n","yolo_model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")\n","\n","# since we are only intrested in detecting person\n","yolo_model.classes = [0]\n","\n","mp_drawing = mp.solutions.drawing_utils\n","mp_pose = mp.solutions.pose"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s2COKNw7mweu","executionInfo":{"status":"ok","timestamp":1739645929914,"user_tz":180,"elapsed":2136,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}},"outputId":"d666eb6c-4323-483a-ff50-fa276db9da6f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n","YOLOv5 🚀 2025-2-15 Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n","Adding AutoShape... \n"]}]},{"cell_type":"code","source":["def processar_video_multiplas_pessoas(input_video_path, output_video_path):\n","\n","  cap = cv2.VideoCapture(input_video_path)\n","\n","  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","  fps = int(cap.get(cv2.CAP_PROP_FPS))\n","  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","  out = cv2.VideoWriter(\n","      output_video_path,\n","      cv2.VideoWriter_fourcc(*\"mp4v\"),\n","      fps,\n","      (frame_width, frame_height),\n","  )\n","\n","  for _ in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n","      ret, frame = cap.read()\n","      if not ret:\n","          break\n","\n","      # Recolor Feed from RGB to BGR\n","      image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      # making image writeable to false improves prediction\n","      image.flags.writeable = False\n","\n","      result = yolo_model(image)\n","\n","      # Recolor image back to BGR for rendering\n","      image.flags.writeable = True\n","      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","      # This array will contain crops of images incase we need it\n","      img_list = []\n","\n","      # we need some extra margin bounding box for human crops to be properly detected\n","      MARGIN = 10\n","\n","      for xmin, ymin, xmax, ymax, confidence, clas in result.xyxy[0].tolist():\n","          with mp_pose.Pose(\n","              static_image_mode=False,\n","              model_complexity=2,\n","              smooth_landmarks=True,\n","              min_detection_confidence=0.4, min_tracking_confidence=0.4\n","          ) as pose:\n","              # Media pose prediction ,we are\n","              results = pose.process(\n","                  image[\n","                      int(ymin) + MARGIN : int(ymax) + MARGIN,\n","                      int(xmin) + MARGIN : int(xmax) + MARGIN :,\n","                  ]\n","              )\n","\n","              # Draw landmarks on image, if this thing is confusing please consider going through numpy array slicing\n","              mp_drawing.draw_landmarks(\n","                  image[\n","                      int(ymin) + MARGIN : int(ymax) + MARGIN,\n","                      int(xmin) + MARGIN : int(xmax) + MARGIN :,\n","                  ],\n","                  results.pose_landmarks,\n","                  mp_pose.POSE_CONNECTIONS,\n","                  mp_drawing.DrawingSpec(\n","                      color=(245, 117, 66), thickness=2, circle_radius=2\n","                  ),\n","                  mp_drawing.DrawingSpec(\n","                      color=(245, 66, 230), thickness=2, circle_radius=2\n","                  ),\n","              )\n","              img_list.append(image[int(ymin) : int(ymax), int(xmin) : int(xmax) :])\n","\n","      out.write(image)\n","\n","  cap.release()\n","  out.release()\n","  cv2.destroyAllWindows()"],"metadata":{"id":"Vn3S3-nchawh","executionInfo":{"status":"ok","timestamp":1739643756996,"user_tz":180,"elapsed":452,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Caminhos para vídeo de entrada/saída\n","input_video_path = \"/content/drive/MyDrive/Colab Notebooks/F4/input_end.mp4\"  # Substituir pelo caminho do vídeo carregado\n","output_video_path = \"/content/drive/MyDrive/Colab Notebooks/F4/output_end_poses.mp4\"\n","\n","\n","# Execute a função para analisar o vídeo\n","processar_video_multiplas_pessoas(input_video_path, output_video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2x2wsi8av3R","executionInfo":{"status":"ok","timestamp":1739644806635,"user_tz":180,"elapsed":1034573,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}},"outputId":"cef46cbc-9baa-48d4-b340-4f76927e74f1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Processando vídeo: 100%|██████████| 1576/1576 [17:13<00:00,  1.52it/s]\n"]}]},{"cell_type":"code","source":["# Função para detectar se o braço está levantado\n","def detecta_braco_levantado(landmarks):\n","    \"\"\"\n","    Detecta se a pessoa levantou o braço.\n","\n","    Args:\n","        landmarks: Lista de landmarks do Mediapipe.\n","\n","    Returns:\n","        True se o braço está levantado, False caso contrário.\n","    \"\"\"\n","    ombro_direito = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n","    ombro_esquerdo = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n","    mao_direita = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n","    mao_esquerda = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n","\n","    # Define uma altura acima do ombro para considerar o braço levantado\n","    altura_acima_ombro = 0.1  # Ajuste este valor conforme necessário\n","\n","    if mao_direita.y < ombro_direito.y - altura_acima_ombro or \\\n","      mao_esquerda.y < ombro_esquerdo.y - altura_acima_ombro:\n","        return True\n","    else:\n","          return False\n","\n","# Função para detectar giro do corpo (simplificada)\n","#  **IMPORTANTE:** Essa detecção de giro é muito simplificada e provavelmente\n","#  precisará de ajustes e refinamentos significativos para funcionar bem\n","#  em diferentes cenários.  Detectar um giro completo de forma robusta\n","#  é um problema complexo.\n","def detecta_giro(landmarks, historico_ombros, threshold=0.1):\n","    \"\"\"\n","    Detecta se a pessoa girou o corpo.  **ESTA É UMA IMPLEMENTAÇÃO SIMPLIFICADA.**\n","\n","    Args:\n","        landmarks: Lista de landmarks do Mediapipe.\n","        historico_ombros: Lista com as posições dos ombros em frames anteriores.\n","        threshold: Limiar para detectar a mudança na posição dos ombros.\n","\n","    Returns:\n","        True se um giro for detectado, False caso contrário.\n","    \"\"\"\n","\n","    ombro_direito = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n","    ombro_esquerdo = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n","\n","    # Adiciona a posição atual dos ombros ao histórico\n","    historico_ombros.append(((ombro_direito.x, ombro_direito.y), (ombro_esquerdo.x, ombro_esquerdo.y)))\n","\n","    # Mantém o histórico com um tamanho máximo (por exemplo, os últimos 10 frames)\n","    if len(historico_ombros) > 10:\n","        historico_ombros.pop(0)\n","\n","    # Se não tivermos histórico suficiente, não podemos detectar o giro\n","    if len(historico_ombros) < 5: # Ajuste este valor conforme necessário\n","        return False\n","\n","    # Calcula a diferença na posição dos ombros entre o frame atual e o frame anterior\n","    ombro_direito_anterior = historico_ombros[-2][0]\n","    ombro_esquerdo_anterior = historico_ombros[-2][1]\n","\n","    diff_ombro_direito_x = ombro_direito.x - ombro_direito_anterior[0]\n","    diff_ombro_esquerdo_x = ombro_esquerdo.x - ombro_esquerdo_anterior[0]\n","\n","\n","    # Se a diferença na posição dos ombros for maior que um limiar, consideramos que houve um giro\n","    if abs(diff_ombro_direito_x) > threshold or abs(diff_ombro_esquerdo_x) > threshold:\n","        return True\n","    else:\n","        return False"],"metadata":{"id":"yg55oX4M_ElM","executionInfo":{"status":"ok","timestamp":1739646220782,"user_tz":180,"elapsed":1069,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Caminhos para vídeo de entrada/saída\n","input_video_path = \"/content/drive/MyDrive/Colab Notebooks/F4/input_end.mp4\"  # Substituir pelo caminho do vídeo carregado\n","output_video_path = \"/content/drive/MyDrive/Colab Notebooks/F4/output_end_poses.mp4\"\n","\n","# Inicialização do Mediapipe\n","mp_pose = mp.solutions.pose\n","pose = mp_pose.Pose(\n","    static_image_mode=False,\n","    model_complexity=1,\n","    smooth_landmarks=True,\n","    enable_segmentation=False,\n","    smooth_segmentation=True,\n","    min_detection_confidence=0.5,\n","    min_tracking_confidence=0.5,\n",")\n","\n","# Carrega o vídeo\n","cap = cv2.VideoCapture(input_video_path)\n","if not cap.isOpened():\n","    print(\"Erro ao abrir o vídeo.\")\n","    exit()\n","\n","# Obtém informações do vídeo para criar o arquivo de saída\n","frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","# Define o codec e cria o objeto VideoWriter para salvar o vídeo\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v') # ou 'XVID'\n","out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","historico_ombros = []  # Histórico para detecção de giro\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Converte a imagem para RGB\n","    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    image.flags.writeable = False\n","\n","    # Processa a imagem com o Mediapipe\n","    results = pose.process(image)\n","\n","    # Converte a imagem de volta para BGR\n","    image.flags.writeable = True\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","    # Desenha os landmarks na imagem (se detectados)\n","    if results.pose_landmarks:\n","        mp.drawing_utils.draw_landmarks(\n","            image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n","\n","        # Imprime os landmarks (opcional, pode ser muito verbose)\n","        # print(\"Landmarks:\", results.pose_landmarks.landmark)\n","\n","        # Detecta atividades\n","        landmarks = results.pose_landmarks.landmark\n","        if detecta_braco_levantado(landmarks):\n","            print(\"Atividade: Levantou o braço\")\n","            cv2.putText(image, \"Levantou o braco\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)  # Adiciona texto no vídeo\n","\n","        if detecta_giro(landmarks, historico_ombros):\n","            print(\"Atividade: Girou o corpo\")\n","            cv2.putText(image, \"Girou o corpo\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)  # Adiciona texto no vídeo\n","\n","\n","    # Salva o frame no vídeo de saída\n","    out.write(image)\n","\n","    # Sai do loop se a tecla 'q' for pressionada\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","# Libera os recursos\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()\n","\n","print(\"Vídeo processado e salvo em:\", output_video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"2wFs4yc29HRn","executionInfo":{"status":"error","timestamp":1739646226363,"user_tz":180,"elapsed":2746,"user":{"displayName":"Michael Marques","userId":"04742422815502680845"}},"outputId":"ba1e3279-ca13-415e-9e5d-e318e419292a"},"execution_count":22,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'mediapipe' has no attribute 'drawing_utils'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b01acf31dd05>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Desenha os landmarks na imagem (se detectados)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         mp.drawing_utils.draw_landmarks(\n\u001b[0m\u001b[1;32m     53\u001b[0m             image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'mediapipe' has no attribute 'drawing_utils'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gETT_UrY9Vo7"},"execution_count":null,"outputs":[]}]}